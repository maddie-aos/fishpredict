{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with importing all packages needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.affinity import scale\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, MultiPoint\n",
    "import pandas as pd\n",
    "from pygbif.species import name_backbone\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from numpy import shape\n",
    "from pathlib import Path\n",
    "file_dir=('/Users/maddie/Projects/CPSC_597/data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of pre-processing is to then create a dictonary of all the unique species from the main CSV and separate them into data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframes to be concatenated and filtered\n",
    "occ_all_species = pd.read_csv(file_dir+\"/data_raw/gbif_data_raw/occurrences_all_species.csv\", low_memory= False)\n",
    "df = occ_all_species[occ_all_species['label'].str.contains(\" \")]\n",
    "\n",
    "#Get unique label names\n",
    "unique_labels=df[\"label\"].unique()\n",
    "\n",
    "names = []\n",
    "back_key =[]\n",
    "remaining_labels=[]\n",
    "\n",
    "#Get backbone associated species names and taxon keys\n",
    "for item in unique_labels:\n",
    "    if \"species\" in name_backbone(item):\n",
    "        i = name_backbone(item)['species']\n",
    "        j = name_backbone(item)['speciesKey']\n",
    "        names.append(i)\n",
    "        back_key.append(j)\n",
    "    else:\n",
    "        remaining_labels.append(item)\n",
    "        \n",
    "for item in remaining_labels:\n",
    "    value=name_backbone(item)['taxonKey']\n",
    "    back_key.append(value)\n",
    "    names.append(item)\n",
    "    \n",
    "#Put into DataFrame\n",
    "df=pd.DataFrame({\"label\": unique_labels,\"back_key\": back_key,\"species\": names},columns=[\"label\",\"back_key\",\"species\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all rows from the dataframes that do not have an entry in either the latitude column, the longitude column, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 without na's n.rows: 250264\n"
     ]
    }
   ],
   "source": [
    "#Concatenate with occurrence data, dataframe, drop na's \n",
    "df2=pd.merge(occ_all_species,df,how=\"left\",on=\"label\")\n",
    "\n",
    "df2 = df2[pd.notnull(df2['species_x'])]\n",
    "df2 = df2[pd.notnull(df2['decimalLatitude'])]\n",
    "df2 = df2[pd.notnull(df2['decimalLongitude'])]\n",
    "\n",
    "print(\"df2 without na's n.rows:\", len(df2.index))\n",
    "\n",
    "df2[\"back_key\"]=df2[\"back_key\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out all species that made it through the first check, and the number of entries per species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citharichthys_sordidus 31634\n",
      "Engraulis_mordax 24710\n",
      "Paralichthys_californicus 5340\n",
      "Scomber_japonicus 20855\n",
      "Thunnus_alalunga 35567\n",
      "Xiphias_gladius 132158\n"
     ]
    }
   ],
   "source": [
    "#list of species\n",
    "species = df2[\"species_x\"].unique()\n",
    "species.sort()\n",
    "\n",
    "#save separate dataframe for each species as csv file \n",
    "for spec in species:\n",
    "    data=df2.loc[df2['species_x'] == spec]\n",
    "    if len(data.index)>= 10:\n",
    "        spec=spec.replace(\" \",\"_\")\n",
    "        print(\"%s\"%spec, len(data.index))\n",
    "        data.to_csv(file_dir+'/data_raw/gbif_data_raw/%s_gbif_raw.csv'%spec)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all of the Species are written into a list, then that list is used to select what dataframe is being used. From there all entries are filtered based on the following two critera: \n",
    "\n",
    "1. All latitude and longitude entries that have less than two decimal places are dropped.\n",
    "2. All repeat entries are dropped. \n",
    "\n",
    "From there, the final list of each species and the remaining entries that match the criterion listed above are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing species Citharichthys_sordidus\n",
      "length only including lon-lat 2 decimals 31346\n",
      "7674\n",
      "processing species Engraulis_mordax\n",
      "length only including lon-lat 2 decimals 23730\n",
      "4841\n",
      "processing species Paralichthys_californicus\n",
      "length only including lon-lat 2 decimals 5184\n",
      "3199\n",
      "processing species Scomber_japonicus\n",
      "length only including lon-lat 2 decimals 19433\n",
      "6693\n",
      "processing species Thunnus_alalunga\n",
      "length only including lon-lat 2 decimals 24982\n",
      "14918\n",
      "processing species Xiphias_gladius\n",
      "length only including lon-lat 2 decimals 80352\n",
      "50913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create txt file with name of species included after filtering\n",
    "taxa_list=open(file_dir+'/data_raw/gbif_data_raw/taxa_list.txt',\"w\")\n",
    "\n",
    "\n",
    "#Filter occurrences per species\n",
    "for spec in species:\n",
    "    \n",
    "    data=df2.loc[df2['species_x'] == spec] #select subset of species\n",
    "    \n",
    "    # check >10 observations\n",
    "    if len(data.index)>= 10: \n",
    "\n",
    "        spec = spec.replace(\" \",\"_\")\n",
    "        print(\"processing species %s\"%spec)\n",
    "\n",
    "        data=pd.read_csv(file_dir+'/data_raw/gbif_data_raw/%s_gbif_raw.csv'%spec, low_memory= False) #load in data\n",
    "        \n",
    "        ###################################################\n",
    "        # check number of decimals longitude and latitude #\n",
    "        ###################################################\n",
    "        str_lat=(pd.Series.tolist(data[\"decimalLatitude\"].astype(str)))\n",
    "        str_lon=(pd.Series.tolist(data[\"decimalLongitude\"].astype(str)))\n",
    "        dec_lat=[]\n",
    "        dec_lon=[]\n",
    "\n",
    "        for i in range(len(str_lat)):\n",
    "    \n",
    "            if \"e\" in str_lat[i]:\n",
    "                str_lat[i]=\"0.00\"\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                dec_lat.append(int(len(decla)))\n",
    "            else:\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                dec_lat.append(int(len(decla)))\n",
    "#                \n",
    "        for i in range(len(str_lon)):\n",
    "            declo=str_lon[i].split(\".\")[1]\n",
    "            dec_lon.append(int(len(declo)))\n",
    "#    \n",
    "        data[\"dec_lat\"]=dec_lat\n",
    "        data[\"dec_lon\"]=dec_lon\n",
    "\n",
    "        # filter only include those with min. 2 points\n",
    "        data=data[data[\"dec_lat\"] >= 2]\n",
    "        data=data[data[\"dec_lon\"] >= 2]\n",
    "        print(\"length only including lon-lat 2 decimals\",len(data.index))\n",
    "#\n",
    "        data['coordinates'] = list(zip(data[\"decimalLongitude\"], data[\"decimalLatitude\"]))\n",
    "        data['lonlat'] = list(zip(data[\"decimalLongitude\"], data[\"decimalLatitude\"]))\n",
    "        data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        # only keep records with unique lon-lat #\n",
    "        #########################################\n",
    "        \n",
    "        data = data.drop_duplicates('lonlat')\n",
    "       \n",
    "        \n",
    "        # check >10 observations\n",
    "        if len(data.index)>=10:\n",
    "            #save to csv\n",
    "            data.to_csv(file_dir+'/modified_data/gbif_filtered/%s_filtered_data.csv'%spec)\n",
    "            taxa_list.write(spec+\"\\n\")\n",
    "            print(len(data))\n",
    "\n",
    "#close text file\n",
    "taxa_list.close()\n",
    "# next species!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
