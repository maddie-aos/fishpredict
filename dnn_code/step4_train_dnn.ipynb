{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84a68607",
   "metadata": {},
   "source": [
    "Starting with importing all packages needed for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d06875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 19:47:45.293751: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "import math\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import regularizers\n",
    "import pickle\n",
    "\n",
    "    \n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b49fa13",
   "metadata": {},
   "source": [
    "Reading in the name of all environmental variables and creating a new file which will display all the metrics of the best performing model for each species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eebb65e-954d-4f38-a8b8-60d8627f46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir=('/Users/maddie/Projects/fishpredict')\n",
    "#create text file to store results in and close again:\n",
    "with open(file_dir+'/results/DNN_performance/DNN_eval.txt','w+') as file:\n",
    "    file.write(\"Species\"+\"\\t\"+\"Test_loss\"+\"\\t\"+\"Test_acc\"+\"\\t\"+\"Test_tpr\"+\"\\t\"+\"Test_AUC\"+\"\\t\"+\"Test_LCI95%\"+\"\\t\"+\"Test_UCI95%\"+\"\\t\"+\"occ_samples\"+\"\\t\"+\"abs_samples\"+\"\\n\")\n",
    "    file.close()\n",
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+'/data/modified_data/gbif_filtered/taxa_list.txt',header=None)\n",
    "taxa.columns=[\"taxon\"] \n",
    "\n",
    "###column variable names\n",
    "with open(file_dir+'/data/data_raw/bio-oracle/variable_list.txt') as f:\n",
    "      new_cols = f.readlines()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5924a2e6",
   "metadata": {},
   "source": [
    "Constructing and testing the model for each species. \n",
    "\n",
    "* For every species that I had, I constructed and trained the model 5 times, and saved the model with the best metrics for each species. \n",
    "  \n",
    "* During each of the 5 runs per species, there were 40 epochs per model, using the ADAM optimizer and a learning rate of 0.001. \n",
    "  \n",
    "* All species data was split 85/15 for training and 'evaluating' (valiation, but it is shown as testing here). \n",
    "  \n",
    "* The model with the best AUC and other metrics was saved in an .h5 model which was used for both testing in step 5 and for predicitons in the web app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be66969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "253/253 [==============================] - 0s 1ms/step - loss: 0.3400 - accuracy: 0.8584\n",
      "253/253 [==============================] - 0s 969us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 2\n",
      "253/253 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8703\n",
      "253/253 [==============================] - 0s 855us/step\n",
      "run 3\n",
      "253/253 [==============================] - 0s 870us/step - loss: 0.3408 - accuracy: 0.8584\n",
      "253/253 [==============================] - 0s 784us/step\n",
      "run 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8_/ndsj9vc92xq7fmvlqlkg8zt40000gn/T/ipykernel_33210/2210747971.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Train model #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1799\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[1;32m   1800\u001b[0m                             \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m                             \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1407\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         ):\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m             \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m             can_run_full_execution = (\n\u001b[1;32m   1413\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    691\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    308\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;31m# Make sure we get an input with handle data attached from resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_data\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4185\u001b[0m         _ctx, \"Identity\", name, input)\n\u001b[1;32m   4186\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4188\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4189\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4190\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4191\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4192\u001b[0m       return identity_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "var_names=[]\n",
    "\n",
    "for item in new_cols:\n",
    "    item=item.replace(\"\\n\",\"\")\n",
    "    var_names.append(item) \n",
    "\n",
    "    \n",
    "for species in taxa[\"taxon\"][:]:\n",
    "   \n",
    "    #open dataframe and rename columns\n",
    "    spec = species\n",
    "    table = pd.read_csv(file_dir +\"/data/modified_data/spec_ppa_env/%s_env_dataframe.csv\"%spec)         \n",
    "    table.rename(columns=dict(zip(table.columns[1:11], var_names)),inplace=True)\n",
    "    \n",
    "    ####################################\n",
    "    #  filter dataframe for training   #\n",
    "    ####################################\n",
    "   \n",
    "    # drop any row with no-data values\n",
    "    table = table.dropna(axis=0, how=\"any\")\n",
    "\n",
    "\n",
    "    # make feature vector\n",
    "    band_columns = [column for column in table.columns[1:11]]\n",
    "    \n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for _, row in table.iterrows():\n",
    "        x = row[band_columns].values\n",
    "        x = x.tolist()\n",
    "        x.append(row[\"present/pseudo_absent\"])\n",
    "        X.append(x)\n",
    "\n",
    "    df = pd.DataFrame(data=X, columns=band_columns + [\"presence\"])\n",
    "    df.to_csv(\"filtered.csv\", index=None)\n",
    "\n",
    "    # extract n. of occ. and abs. samples\n",
    "    occ_len=int(len(df[df[\"presence\" ]==1]))\n",
    "    abs_len=int(len(df[df[\"presence\" ]==0]))\n",
    "    \n",
    "    ####################################\n",
    "    #  Numpy feature and target array  #\n",
    "    ####################################\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    band_columns = [column for column in df.columns[:-1]]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        X.append(row[band_columns].values.tolist())\n",
    "        y.append([1 - row[\"presence\"], row[\"presence\"]])\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ####################################\n",
    "    #    Split training and test set   #\n",
    "    ####################################\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y,random_state=42)\n",
    "    \n",
    "    test_set=pd.DataFrame(X_test)\n",
    "    test_set.rename(columns=dict(zip(test_set.columns[0:11], var_names)),inplace=True)\n",
    "    \n",
    "    shuffled_X_train=X_train.copy()\n",
    "    np.random.shuffle(shuffled_X_train)\n",
    "    shuffled_X_train=shuffled_X_train[:1000] # random subsample from test set for feature importance\n",
    "    \n",
    "    shuffled_X_test=X_test.copy()\n",
    "    np.random.shuffle(shuffled_X_test)\n",
    "    shuffled_X_test=shuffled_X_test[:1000] # random subsample from test set for feature importance\n",
    "    \n",
    "    ####################################\n",
    "    #      Training and testing        #\n",
    "    ####################################\n",
    "    \n",
    "    # prepare metrics\n",
    "    test_loss=[]\n",
    "    test_acc=[]\n",
    "    test_AUC=[]\n",
    "    test_tpr=[]\n",
    "    test_uci=[]\n",
    "    test_lci=[]\n",
    "\n",
    "   \n",
    "    Best_model_AUC=[0]\n",
    "    \n",
    "    # Five repetitions\n",
    "    for i in range(1,6):\n",
    "        print(\"run %s\"%i)\n",
    "        ###################\n",
    "        # Construct model #\n",
    "        ###################\n",
    "        batch_size = 75\n",
    "        num_classes = 2\n",
    "        epochs = 40\n",
    "\n",
    "        num_inputs = X.shape[1]  # number of features\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        layer_1 = Dense(100, activation='relu',input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "        layer_2 = Dense(50, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "        layer_3 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.0000001))\n",
    "        layer_4 = Dense(10, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.00000001))\n",
    "\n",
    "\n",
    "        model.add(layer_1)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_2)\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(layer_3)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_4)\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "        out_layer = Dense(num_classes, activation=None)\n",
    "        model.add(out_layer)\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics =['accuracy'])\n",
    "        \n",
    "        ###############\n",
    "        # Train model #\n",
    "        ###############\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        #history = model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, verbose=0)\n",
    "        #shap_values = shap.Explainer(model)(X_test)\n",
    "\n",
    "        ##############\n",
    "        # Test model #\n",
    "        ##############\n",
    "        score = model.evaluate(X_test, y_test, verbose=1)\n",
    "        predictions = model.predict(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:, 1], predictions[:, 1])\n",
    "        len_tpr=int(len(tpr)/2)\n",
    "      \n",
    "\n",
    "        #################\n",
    "        # Append scores #\n",
    "        #################\n",
    "        test_loss.append(score[0])\n",
    "        test_acc.append(score[1])\n",
    "        test_AUC.append(roc_auc_score(y_test[:, 1], predictions[:, 1]))\n",
    "        test_tpr.append(tpr[len_tpr])\n",
    "        AUC = roc_auc_score(y_test[:, 1], predictions[:, 1])\n",
    "\n",
    "        ###############################\n",
    "        # Create confidence intervals #\n",
    "        ###############################\n",
    "        n_bootstraps=1000\n",
    "        y_pred=predictions[:,1]\n",
    "        y_true=y_test[:,1]\n",
    "        rng_seed=42\n",
    "        bootstrapped_scores =[]\n",
    "\n",
    "\n",
    "        rng=np.random.RandomState(rng_seed)\n",
    "        for i in range (n_bootstraps):\n",
    "            #bootstrap by sampling with replacement on prediction indices\n",
    "            indices = rng.randint(0,len(y_pred)-1,len(y_pred))\n",
    "            if len (np.unique(y_true[indices])) <2:\n",
    "                continue\n",
    "\n",
    "            score = roc_auc_score(y_true[indices],y_pred[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "\n",
    "        sorted_scores=np.array(bootstrapped_scores)\n",
    "        sorted_scores.sort()\n",
    "\n",
    "        ci_lower=sorted_scores[int(0.05*len(sorted_scores))]\n",
    "        ci_upper=sorted_scores[int(0.95*len(sorted_scores))]\n",
    "     \n",
    "        test_lci.append(ci_lower)\n",
    "        test_uci.append(ci_upper)\n",
    "       \n",
    "    \n",
    "        ##############################################################\n",
    "        # Selection of best model across runs and feature importance #\n",
    "        ##############################################################\n",
    "    \n",
    "        #determine whether new model AUC is higher\n",
    "        if AUC > Best_model_AUC[0]:\n",
    "            # if yes save model to disk / overwrite previous model\n",
    "            Best_model_AUC[0]=AUC\n",
    "            model_filename = os.path.join(file_dir, 'pickled_model', '{}.pkl'.format(spec))\n",
    "            os.makedirs(os.path.dirname(model_filename), exist_ok=True)\n",
    "            with open(model_filename, 'wb') as model_file:\n",
    "                pickle.dump(model, model_file)\n",
    " \n",
    "            \n",
    "            if int(len(X_train)) > 5000:           \n",
    "                explainer=shap.DeepExplainer(model,shuffled_X_train)\n",
    "                test_set=pd.DataFrame(shuffled_X_test)\n",
    "                test_set.rename(columns=dict(zip(test_set.columns[0:40], var_names)),inplace=True)\n",
    "                \n",
    "                shap_values=explainer.shap_values(shuffled_X_test)\n",
    "                fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "                plt.savefig(file_dir+'/results/fish/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            \n",
    "            else:\n",
    "                explainer=shap.DeepExplainer(model,X_train)\n",
    "                shap_values=explainer.shap_values(X_test)\n",
    "                fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "                plt.savefig(file_dir+'/results/fish/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            \n",
    "\n",
    "\n",
    "    # Model output metrics averaged across five runs to be written to file\n",
    "    avg_loss= sum(test_loss)/len(test_loss)\n",
    "    avg_acc = sum(test_acc)/len(test_acc)\n",
    "    avg_AUC = sum(test_AUC)/len(test_AUC)\n",
    "    avg_tpr = sum(test_tpr)/len(test_tpr)\n",
    "    avg_lci = sum(test_lci)/len(test_lci)\n",
    "    avg_uci = sum(test_uci)/len(test_uci)\n",
    "\n",
    "    #Write to file\n",
    "    with open(file_dir+'/results/DNN_performance/DNN_eval.txt','a') as file:\n",
    "        file.write(spec+\"\\t\"+str(avg_loss)+\"\\t\"+str(avg_acc)+\"\\t\"+str(avg_tpr)+\"\\t\"+str(avg_AUC)+\"\\t\"+str(avg_lci)+\"\\t\"+str(avg_uci)+\"\\t\"+str(occ_len)+\"\\t\"+str(abs_len)+\"\\n\")       \n",
    "\n",
    "\n",
    "    #Next species!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
